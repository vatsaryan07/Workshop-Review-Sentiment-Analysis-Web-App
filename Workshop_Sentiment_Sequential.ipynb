{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Workshop_Sentiment_Sequential.ipynb",
      "provenance": [],
      "mount_file_id": "1OMh-TbT7Ss3wY_sXIMh6WSndt0uYqIRS",
      "authorship_tag": "ABX9TyOGfouynU8wyjSk5haBS9he"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Qafaye4C2Ua",
        "outputId": "f2c45302-c968-4d5c-957a-c2d7701dc5e2"
      },
      "source": [
        "#Importing dependencies\n",
        "\n",
        "import nltk\n",
        "import random\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords  \n",
        "from nltk.tokenize import word_tokenize  "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "hHz0Gv72C4-v",
        "outputId": "1b5ecc61-af4b-43df-8c6e-5a63592f5e45"
      },
      "source": [
        "\n",
        "# Using the kaggle token for accessing the IMDB dataset\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4a66fd3c-fd47-42f7-a3c5-90b19e0a35aa\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4a66fd3c-fd47-42f7-a3c5-90b19e0a35aa\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9SSq0ibEQj_",
        "outputId": "67002ae2-a393-4756-a91a-f4ca8d04b3fa"
      },
      "source": [
        "!ls -lha kaggle.json\n",
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
        "!unzip imdb-dataset-of-50k-movie-reviews.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 67 Mar 25 06:26 kaggle.json\n",
            "Downloading imdb-dataset-of-50k-movie-reviews.zip to /content\n",
            " 86% 22.0M/25.7M [00:00<00:00, 111MB/s]\n",
            "100% 25.7M/25.7M [00:00<00:00, 102MB/s]\n",
            "Archive:  imdb-dataset-of-50k-movie-reviews.zip\n",
            "  inflating: IMDB Dataset.csv        \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4jiYVSKxEBS"
      },
      "source": [
        "# Reading the csv file and replacing positive and negative with 1 and 0\n",
        "orig_data = pd.read_csv('IMDB Dataset.csv')\n",
        "orig_data.replace({\"positive\":1,\"negative\":0},inplace=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_ZV5rDSIGD2"
      },
      "source": [
        "# Removing 'not' from stopwords' list and adding the <br> token\n",
        "stop_words = list(set(stopwords.words('english')))\n",
        "stop_words.remove('not')\n",
        "stop_words.append('br')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6m5gdyQZK6_r",
        "outputId": "6058d5c3-33e7-4f95-f08b-2115089eea6a"
      },
      "source": [
        "stop_words"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['doesn',\n",
              " 'about',\n",
              " 'from',\n",
              " 'during',\n",
              " 'out',\n",
              " 'you',\n",
              " \"doesn't\",\n",
              " 'very',\n",
              " 'our',\n",
              " 'it',\n",
              " 'do',\n",
              " 'mustn',\n",
              " \"it's\",\n",
              " 'o',\n",
              " 'won',\n",
              " 'myself',\n",
              " 'which',\n",
              " 'this',\n",
              " 'him',\n",
              " 'd',\n",
              " 'on',\n",
              " 'when',\n",
              " 'did',\n",
              " 'than',\n",
              " 'm',\n",
              " 'i',\n",
              " 'its',\n",
              " \"aren't\",\n",
              " 'had',\n",
              " 'he',\n",
              " 'because',\n",
              " 'in',\n",
              " 'should',\n",
              " \"you've\",\n",
              " 'such',\n",
              " 'her',\n",
              " 'ain',\n",
              " \"weren't\",\n",
              " 'his',\n",
              " 'down',\n",
              " 'each',\n",
              " 'who',\n",
              " 'no',\n",
              " 'own',\n",
              " 'yours',\n",
              " 'so',\n",
              " 'until',\n",
              " \"shouldn't\",\n",
              " 'weren',\n",
              " 'a',\n",
              " 'don',\n",
              " 'been',\n",
              " 'as',\n",
              " \"won't\",\n",
              " 'above',\n",
              " 'any',\n",
              " \"don't\",\n",
              " 'we',\n",
              " 'ours',\n",
              " 'just',\n",
              " 'haven',\n",
              " 'mightn',\n",
              " 'only',\n",
              " 'my',\n",
              " 'being',\n",
              " 'does',\n",
              " \"that'll\",\n",
              " 'they',\n",
              " 'has',\n",
              " 'couldn',\n",
              " 'through',\n",
              " 'yourself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'll',\n",
              " 't',\n",
              " 'over',\n",
              " \"mustn't\",\n",
              " 'more',\n",
              " \"you're\",\n",
              " 'doing',\n",
              " 'why',\n",
              " 'was',\n",
              " 'other',\n",
              " 'himself',\n",
              " \"hasn't\",\n",
              " 'is',\n",
              " 've',\n",
              " 'themselves',\n",
              " 'these',\n",
              " 'now',\n",
              " 'while',\n",
              " \"isn't\",\n",
              " 'there',\n",
              " 'isn',\n",
              " \"you'll\",\n",
              " 'what',\n",
              " 'if',\n",
              " 'again',\n",
              " \"you'd\",\n",
              " 'all',\n",
              " 'an',\n",
              " 'are',\n",
              " 'under',\n",
              " 'am',\n",
              " 'with',\n",
              " 'against',\n",
              " 'into',\n",
              " 'once',\n",
              " 'how',\n",
              " 's',\n",
              " 'ourselves',\n",
              " 'between',\n",
              " 'up',\n",
              " 'were',\n",
              " 'both',\n",
              " 'herself',\n",
              " \"mightn't\",\n",
              " 'of',\n",
              " 'hasn',\n",
              " 'those',\n",
              " 'will',\n",
              " 'your',\n",
              " 'after',\n",
              " \"couldn't\",\n",
              " 'where',\n",
              " 'too',\n",
              " 'hadn',\n",
              " 'shan',\n",
              " 'hers',\n",
              " 'then',\n",
              " \"should've\",\n",
              " 'or',\n",
              " \"hadn't\",\n",
              " 'whom',\n",
              " 'can',\n",
              " 'y',\n",
              " 'having',\n",
              " 'have',\n",
              " 'for',\n",
              " 'before',\n",
              " 'some',\n",
              " \"haven't\",\n",
              " 'the',\n",
              " \"didn't\",\n",
              " 'ma',\n",
              " 'needn',\n",
              " 'aren',\n",
              " 'below',\n",
              " 'but',\n",
              " 'wasn',\n",
              " 'further',\n",
              " 'most',\n",
              " 'few',\n",
              " 'at',\n",
              " 're',\n",
              " 'them',\n",
              " 'their',\n",
              " 'be',\n",
              " 'that',\n",
              " \"needn't\",\n",
              " 'yourselves',\n",
              " 'by',\n",
              " 'wouldn',\n",
              " 'same',\n",
              " \"shan't\",\n",
              " 'itself',\n",
              " 'theirs',\n",
              " 'off',\n",
              " 'shouldn',\n",
              " 'didn',\n",
              " \"wouldn't\",\n",
              " 'and',\n",
              " 'me',\n",
              " 'to',\n",
              " \"wasn't\",\n",
              " 'here',\n",
              " 'nor',\n",
              " 'br']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVr6ILNJB_Ml",
        "outputId": "b8b617c8-025e-488d-c21e-5dd41fb95043"
      },
      "source": [
        "# This function takes in data, removes special characters and stop words, returning the X and Y dataset\n",
        "def preprocess_text(data):\n",
        "  x = []\n",
        "  y = []\n",
        "  for p,q in zip(data['review'],data['sentiment']):\n",
        "    cleaned = re.sub(r'[^(a-zA-Z)\\s]','',p)\n",
        "    stopped = '  '.join([w for w in cleaned.split(' ') if w not in stop_words])\n",
        "    x.append(stopped)\n",
        "    y.append(q)\n",
        "  return x,y\n",
        "\n",
        "X,Y = preprocess_text(orig_data)\n",
        "print(len(X))\n",
        "print(len(Y))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000\n",
            "50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yi1ZL0ioMSXQ"
      },
      "source": [
        "# Using the word tokeniser from the keras library's built in preprocessing module to create our bag of words\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokens = keras.preprocessing.text.Tokenizer(\n",
        "    num_words=None,\n",
        "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "    lower=True, split=' ', char_level=False, oov_token='<UNK>'\n",
        ")\n",
        "tokens.fit_on_texts(X)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fR2heFbNSz2",
        "outputId": "e3ed24a2-d55e-417c-cb64-a0bf27ccf03d"
      },
      "source": [
        "len(tokens.word_index)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "171851"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vwFhPy2OBJO"
      },
      "source": [
        "# Converting the text data to sequences utilizing the tokenizer\n",
        "import numpy as np\n",
        "X_fin = tokens.texts_to_sequences(X)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2WQSgkZcG3X"
      },
      "source": [
        "# Padding our data to get uniform length to fit into the model\n",
        "X_fin = keras.preprocessing.sequence.pad_sequences(X_fin,padding = 'post')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WWNUuyV3y3F",
        "outputId": "45cdaee4-4075-4eb6-8766-a7ead4343f49"
      },
      "source": [
        "# Checking the length of the padding for future reference\n",
        "length = [len(k) for k in X_fin]\n",
        "max_pad = max(length)\n",
        "max_pad"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1493"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF89YApMOSft"
      },
      "source": [
        "# Dividing test and train data into an 80-20 split\n",
        "x_train = np.array(X_fin[:40000])\n",
        "y_train = np.array(Y[:40000])\n",
        "x_val = np.array(X_fin[40000:])\n",
        "y_val = np.array(Y[40000:])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwl_vEsfiBp9",
        "outputId": "c6e52eee-aede-4526-d475-862bf9e01591"
      },
      "source": [
        "print(y_train)\n",
        "print(x_train[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 ... 1 0 0]\n",
            "[   7 1886  992 ...    0    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJjSy2XN9qd2",
        "outputId": "8bf032ae-6bb3-4bb7-f76e-5b5097112e63"
      },
      "source": [
        "# Using a sequential model to predict the sentiment\n",
        "from keras.layers import Dense,Bidirectional,LSTM,Input,Embedding,Dropout,BatchNormalization,TimeDistributed\n",
        "from keras.models import Model\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "inputs = Input(shape=(None,), dtype=\"int32\")\n",
        "\n",
        "# Embedding each integer in a 128-dimensional vector (these word embeddings can either be learned by scratch or pre-learnt)\n",
        "x = Embedding(len(tokens.word_index),200)(inputs)\n",
        "\n",
        "# Addding 3 bidirectional LSTMs\n",
        "x = Bidirectional(LSTM(128,return_sequences=True))(x)\n",
        "x = Dropout(0.20)(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = Bidirectional(LSTM(128,return_sequences=True))(x)\n",
        "x = Dropout(0.10)(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = Bidirectional(LSTM(128))(x)\n",
        "\n",
        "# Getting the final output with the sigmoid function\n",
        "outputs = Dense(1, activation=\"sigmoid\")(x)\n",
        "k_model = Model(inputs, outputs)\n",
        "k_model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, None, 200)         34370200  \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, None, 256)         336896    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, None, 256)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, None, 256)         1024      \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, None, 256)         394240    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, None, 256)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, None, 256)         1024      \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 256)               394240    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 35,497,881\n",
            "Trainable params: 35,496,857\n",
            "Non-trainable params: 1,024\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPza7z8SNvHc",
        "outputId": "bac37a32-b2c7-4967-fda5-68a3ca2da1c0"
      },
      "source": [
        "# Using the adam optimization method to train the model on 10 epochs\n",
        "k_model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = k_model.fit(x_train, y_train,batch_size = 32,epochs=10)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1250/1250 [==============================] - 878s 672ms/step - loss: 0.5067 - accuracy: 0.7253\n",
            "Epoch 2/10\n",
            "1250/1250 [==============================] - 842s 674ms/step - loss: 0.1924 - accuracy: 0.9305\n",
            "Epoch 3/10\n",
            "1250/1250 [==============================] - 843s 674ms/step - loss: 0.1226 - accuracy: 0.9572\n",
            "Epoch 4/10\n",
            "1250/1250 [==============================] - 843s 675ms/step - loss: 0.0758 - accuracy: 0.9749\n",
            "Epoch 5/10\n",
            "1250/1250 [==============================] - 844s 675ms/step - loss: 0.0498 - accuracy: 0.9847\n",
            "Epoch 6/10\n",
            "1250/1250 [==============================] - 845s 676ms/step - loss: 0.0358 - accuracy: 0.9893\n",
            "Epoch 7/10\n",
            "1250/1250 [==============================] - 845s 676ms/step - loss: 0.0267 - accuracy: 0.9914\n",
            "Epoch 8/10\n",
            "1250/1250 [==============================] - 845s 676ms/step - loss: 0.0153 - accuracy: 0.9953\n",
            "Epoch 9/10\n",
            "1250/1250 [==============================] - 847s 678ms/step - loss: 0.0146 - accuracy: 0.9956\n",
            "Epoch 10/10\n",
            "1250/1250 [==============================] - 847s 677ms/step - loss: 0.0076 - accuracy: 0.9976\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "E4QPCDFBUX2X",
        "outputId": "2f82e0b8-53ad-4a9d-be01-045116db7964"
      },
      "source": [
        "# Saving the model and downloading it\n",
        "k_model.save('/content/drive/MyDrive/New_model_v2.h5')\n",
        "files.download('New_model_v2.h5')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_e712ba63-7033-479d-9f20-a7a2d7325b80\", \"New_model_v2.h5\", 426085592)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_09Z_in7-NJ"
      },
      "source": [
        "# Saving in drive as backup\n",
        "k_model.save('/content/drive/MyDrive/New_model_v2.h5')"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6maUxi62h7U",
        "outputId": "2701cf26-98b3-4ed9-aadf-e39a50ffa981"
      },
      "source": [
        "# Evaluating the model to check accuracy\n",
        "k_model.evaluate(x_val[:-1],y_val[:-1])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 51s 157ms/step - loss: 0.5922 - accuracy: 0.8728\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5921744704246521, 0.8727872967720032]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TslP9E64k2c"
      },
      "source": [
        "# Defining a function that converts the text into the sequence of padded form\n",
        "def predict(phrase):\n",
        "    phrase = tokens.texts_to_sequences(phrase)\n",
        "    phrase = keras.preprocessing.sequence.pad_sequences(phrase,maxlen=max_pad,padding='post')\n",
        "    res = k_model.predict([phrase])\n",
        "    return res"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYKY_Rmb-lvw"
      },
      "source": [
        "\"\"\"\n",
        "Sample review for \"Avengers: Endgame\" used to check how the model is doing on positive responses: \n",
        "Overwhelming. It best describes the final chapter that culminates Marvel Cinematic Universe’s 21 iconic films into one. And that also describes the experience of watching your favourite superheroes come together for a singular goal, for one last time. Directors Anthony and Joe Russo ensure that the humongous build-up and the avalanche of expectations do not get the better of them. They deliver a largely wholesome product that is full of moments laced with action, emotion, comedy and drama. Writers Christopher Markus and Stephen McFeely take you along, even if you haven’t been following the franchise. They do an incredible job with the screenplay to balance emotions with visual spectacle. So if you’re not a fan yet, chances are, you might become one after watching this instalment.\n",
        "While the screen time for each character is not equal, their significance in the story is. And there are enough surprises in store, as far as their fates are concerned. ‘Endgame’ delivers quite well on the emotional quotient, bringing out superpowers and vulnerabilities of its cinematic demigods through their measured performances. From an upright Captain America (Chris Evans) to a stoic Black Widow (Scarlett Johansson) and from a straight-faced Captain Marvel (Brie Larson) delivering the punches to the reassuring presence of Iron Man (Robert Downey Jr.), ‘Endgame’ has it all and a lot more. Thanks to the conviction in performances, you also might just find yourself rooting for the bad guy, Thanos (Josh Brolin) at some point. However, it’s the comic collective of Thor (Chris Hemsworth), Hulk (Mark Ruffalo) and Ant Man (Paul Rudd) that ensures there are enough lighter moments in this otherwise heartfelt finale.\n",
        "The action becomes progressively intense, but never overbearing. In fact, it remains relevant and true to the narrative, such that it weaves in enough opportunities for major plot twists that even the diehard fans may not see coming. The extensive CGI work adds to the visual appeal, even in 2D.\n",
        "For the non-fans, the film’s explanatory tone might come across as a speed breaker at times, but for the fans, the same invokes hope and excitement, leading to constant gasps and howls.\n",
        "Overall, 'Avengers: Endgame' is a befitting tribute to the Cinematic Universe that has spawned larger-than-life superheroes and super fans. At three hours plus, ‘Endgame’ delivers on a lot of its hallmark promises, leaving its fans with a range of emotions and fond memories.\n",
        "\n",
        "\n",
        "Sample review for \"Disaster Movie\" to see how the model is doing on negative responses:\n",
        "This movie was on TV once so I decided to watch it since I wouldn't have to pay any money for it.The main character Will (played by Matt Lanter) has \n",
        "a dream where he meets a stone age Amy Winehouse (I think it's supposed to be a joke) who tells him that the world is going to end the day this \n",
        "movie premiered in the cinema (Coincidence?) and to stop it they must find a crystal skull. Matt later wakes up to celebrate his super-sweet \n",
        "sixteenth birthday (despite him being in his twenties) in a scene where we get one unfunny joke and celebrity impersonation after another. Then \n",
        "disaster strikes (it seems kinda redundant though since this movie already is one), hurricanes, earthquakes, meteorites and other classic disaster\n",
        "movie ingredients hit planet earth one after another. Will, followed by his friends: Juney (Crista Flanagan), Calvin (Gary \\\"G Thang\\\" Johnson),\n",
        "and Lisa (Kim Kardashian) go out into the city and tries to find his girlfriend and a safe place and later realizes that he has to find the \n",
        "crystal skull to set things right.The problem with this movie is, just like other movies by Jason Friedberg and Aaron Seltzer, that it doesn't\n",
        "stay on the theme but goes all over the place and try to spoof almost every popular movie that was made that year. And I use the term \\\"spoof\\\"\n",
        "lightly. Once again \\\"Seltzerberger\\\" show that they only grasp the most superficial concept of what humor is and never really bother to dig \n",
        "deeper and see what it is that makes things funny. Sometimes doing things outside the theme can work but not if it takes up a majority of the \n",
        "movie. And (for me) this movie is worse than Epic Movie. Yes you read right, Worse than Epic Movie. That movie at least had a story. Sure it was \n",
        "borrowed and \\\"crapified\\\" but at least it was a story. In this movie, everything that happens during the second act, when they try to find a \n",
        "safe place/figure out where they should go, just feels like a filler where the gang stumble into one reference after another. \\\"Seltzerberger\\'s\\\" \n",
        "over-reliance on potty humor, movie/TV references, random musical numbers, deliberately obvious stunt-doubles and crappy special effects does not \n",
        "save them this time.Seltzer and Friedberg, your movie sucks horribly. If I may paraphrase a line from \\'Billy Madison\\' I\\'d like to say: I \n",
        "award you only one star, and may God have mercy on your souls.Once again, if you want to see a GOOD movie made in the style that this train \n",
        "wreck was trying (and failing) to emulate, watch \\\"Hotshots\\\" \\\"Airplane!\\\", \\\"The naked gun\\\" movies, \\\"Top Secret\\\" instead.\"\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUR01rUllEQ3",
        "outputId": "3f2acd4e-0c06-4491-d9de-295ead21a8c0"
      },
      "source": [
        "predict([\"Overwhelming. It best describes the final chapter that culminates Marvel Cinematic Universe’s 21 iconic films into one. And that also describes the experience of watching your favourite superheroes come together for a singular goal, for one last time. Directors Anthony and Joe Russo ensure that the humongous build-up and the avalanche of expectations do not get the better of them. They deliver a largely wholesome product that is full of moments laced with action, emotion, comedy and drama. Writers Christopher Markus and Stephen McFeely take you along, even if you haven’t been following the franchise. They do an incredible job with the screenplay to balance emotions with visual spectacle. So if you’re not a fan yet, chances are, you might become one after watching this instalment. While the screen time for each character is not equal, their significance in the story is. And there are enough surprises in store, as far as their fates are concerned. ‘Endgame’ delivers quite well on the emotional quotient, bringing out superpowers and vulnerabilities of its cinematic demigods through their measured performances. From an upright Captain America (Chris Evans) to a stoic Black Widow (Scarlett Johansson) and from a straight-faced Captain Marvel (Brie Larson) delivering the punches to the reassuring presence of Iron Man (Robert Downey Jr.), ‘Endgame’ has it all and a lot more. Thanks to the conviction in performances, you also might just find yourself rooting for the bad guy, Thanos (Josh Brolin) at some point. However, it’s the comic collective of Thor (Chris Hemsworth), Hulk (Mark Ruffalo) and Ant Man (Paul Rudd) that ensures there are enough lighter moments in this otherwise heartfelt finale. The action becomes progressively intense, but never overbearing. In fact, it remains relevant and true to the narrative, such that it weaves in enough opportunities for major plot twists that even the diehard fans may not see coming. The extensive CGI work adds to the visual appeal, even in 2D. For the non-fans, the film’s explanatory tone might come across as a speed breaker at times, but for the fans, the same invokes hope and excitement, leading to constant gasps and howls. Overall, 'Avengers: Endgame' is a befitting tribute to the Cinematic Universe that has spawned larger-than-life superheroes and super fans. At three hours plus, ‘Endgame’ delivers on a lot of its hallmark promises, leaving its fans with a range of emotions and fond memories.\"\n",
        "])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.9998185]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5YOMITxC61I",
        "outputId": "cc38c224-71ca-465f-bfd4-a288a21a079b"
      },
      "source": [
        "predict([\n",
        "    \"This movie was on TV once so I decided to watch it since I wouldn't have to pay any money for it.The main character Will (played by Matt Lanter) has a dream where he meets a stone age Amy Winehouse (I think it's supposed to be a joke) who tells him that the world is going to end the day this movie premiered in the cinema (Coincidence?) and to stop it they must find a crystal skull. Matt later wakes up to celebrate his super-sweet sixteenth birthday (despite him being in his twenties) in a scene where we get one unfunny joke and celebrity impersonation after another. Then disaster strikes (it seems kinda redundant though since this movie already is one), hurricanes, earthquakes, meteorites and other classic disaster movie ingredients hit planet earth one after another. Will, followed by his friends: Juney (Crista Flanagan), Calvin (Gary \\\"G Thang\\\" Johnson), and Lisa (Kim Kardashian) go out into the city and tries to find his girlfriend and a safe place and later realizes that he has to find the crystal skull to set things right.The problem with this movie is, just like other movies by Jason Friedberg and Aaron Seltzer, that it doesn't stay on the theme but goes all over the place and try to spoof almost every popular movie that was made that year. And I use the term \\\"spoof\\\" lightly. Once again \\\"Seltzerberger\\\" show that they only grasp the most superficial concept of what humor is and never really bother to dig deeper and see what it is that makes things funny. Sometimes doing things outside the theme can work but not if it takes up a majority of the movie. And (for me) this movie is worse than Epic Movie. Yes you read right, Worse than Epic Movie. That movie at least had a story. Sure it was borrowed and \\\"crapified\\\" but at least it was a story. In this movie, everything that happens during the second act, when they try to find a safe place/figure out where they should go, just feels like a filler where the gang stumble into one reference after another. \\\"Seltzerberger\\'s\\\" over-reliance on potty humor, movie/TV references, random musical numbers, deliberately obvious stunt-doubles and crappy special effects does not save them this time.Seltzer and Friedberg, your movie sucks horribly. If I may paraphrase a line from \\'Billy Madison\\' I\\'d like to say: I award you only one star, and may God have mercy on your souls.Once again, if you want to see a GOOD movie made in the style that this train wreck was trying (and failing) to emulate, watch \\\"Hotshots\\\" \\\"Airplane!\\\", \\\"The naked gun\\\" movies, \\\"Top Secret\\\" instead.\"\n",
        "])"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.11181722]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgha6Fzd_4oC"
      },
      "source": [
        "\"\"\"\n",
        "Now checking to see whether we were able to map the intuition to checking for positive/negative workshop reviews : \n",
        "\n",
        "Sample  positive review : \n",
        "\"The workshop was hardly a pushover, it was an amazingly well constructed and well delivered workshop, with key points being covered well,\n",
        "explanations being to the point yet understandable, and all in all was a thrill to be a part of. This was head and shoulders above \n",
        "any other ML workshop I have been on\"\n",
        "\n",
        "Sample negative review : \n",
        "\"\"The workshop was a chore to deal with. Honestly it was boring, drab and much more basic than I expected it to be. I had great expectations \n",
        "but was sadly let down. Poor showing\"\"\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8VcnWX__k5F",
        "outputId": "ff6cb099-5c28-4073-8f33-27a28262abc8"
      },
      "source": [
        "predict([\"The workshop was hardly a pushover, it was an amazingly well constructed and well delivered workshop, with key points being covered well, explanations being to the point yet understandable, and all in all was a thrill to be a part of. This was head and shoulders above any other ML workshop I have been on\"])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.9999167]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKi8SNdLA-uy",
        "outputId": "da906777-09ea-4f0a-b92c-a7d45c906557"
      },
      "source": [
        "predict([\"The workshop was a chore to deal with. Honestly it was boring, drab and much more basic than I expected it to be. I had great expectations but was sadly let down. Poor showing\"])"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.7348706e-05]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAtxINHIwf3r",
        "outputId": "c602a24b-03a7-44ac-af67-5eb569ec3483"
      },
      "source": [
        "# Checking to see if we were able to overcome the trap of \"not good\" = \"bad\"\n",
        "predict(['The workshop was not good at all.'])"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.28421098]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    }
  ]
}